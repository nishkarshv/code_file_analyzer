source,file_name,type,lines_of_code,class_count,function_count,query_count,purpose,dependencies,triggers,sources,sinks,imports,function_names,class_names
,code_analyzer.py,Python Script,304,0,9,0,,"['os', 're', 'pandas', 'ast', 'croniter.croniter', 'pathlib.Path']",[],[],[],"['os', 're', 'pandas', 'ast', 'croniter.croniter', 'pathlib.Path']","['count_lines', 'extract_cron_triggers', 'analyze_pyspark', 'extract_variables', 'extract_function_and_class_names', 'extract_imports_and_counts', 'analyze_sql', 'analyze_file', 'analyze_repo']",[]
,code_file_analyzer copy.py,Python Script,121,0,6,0,,"['os', 're', 'pandas', 'ast', 'croniter.croniter', 'pathlib.Path', 'sqlparse']",[],[],[],"['os', 're', 'pandas', 'ast', 'croniter.croniter', 'pathlib.Path', 'sqlparse']","['count_lines', 'extract_sql_queries', 'analyze_sql', 'extract_cron_triggers', 'analyze_file', 'analyze_repo']",[]
,code_file_analyzer.py,Python Script,392,0,12,1,,"['os', 're', 'pandas', 'ast', 'croniter.croniter', 'pathlib.Path', 'sqlparse']",[],[],[],"['os', 're', 'pandas', 'ast', 'croniter.croniter', 'pathlib.Path', 'sqlparse']","['count_lines', 'extract_sql_queries', 'extract_cron_triggers', 'analyze_pyspark', 'extract_imports', 'analyze_python_file', 'extract_variables', 'extract_function_and_class_names', 'extract_imports_and_counts', 'analyze_sql', 'analyze_file', 'analyze_repo']",[]
,python_rules_check.sh,Shell Script,127,0,0,0,,[],[],[],[],[],[],[]
,python_rules_check_2.sh,Shell Script,83,0,0,0,,[],[],[],[],[],[],[]
,deploy.sh,Shell Script,48,0,0,0,,[],[],[],[],[],[],[]
,crud_operations_bq_dag.py,Python Script,74,0,0,0,,"['airflow.DAG', 'airflow.providers.google.cloud.operators.bigquery.BigQueryCreateEmptyDatasetOperator', 'airflow.utils.dates.days_ago']",[],[],[],"['airflow.DAG', 'airflow.providers.google.cloud.operators.bigquery.BigQueryCreateEmptyDatasetOperator', 'airflow.utils.dates.days_ago']",[],[]
,gcs_to_bq_dag.py,Python Script,74,0,0,0,,"['airflow.DAG', 'airflow.providers.google.cloud.operators.dataproc.DataprocCreateClusterOperator', 'datetime.datetime']",[],[],[],"['airflow.DAG', 'airflow.providers.google.cloud.operators.dataproc.DataprocCreateClusterOperator', 'datetime.datetime']",[],[]
,bq_operations.py,Python Script,98,0,7,3,,"['google.cloud.bigquery', 'time']",[],[],[],"['google.cloud.bigquery', 'time']","['create_dataset', 'create_table', 'insert_data', 'query_data', 'update_data', 'delete_data', 'main']",[]
,gcs_to_bq.py,Python Script,49,0,0,0,PySpark Job,"['from pyspark.sql import SparkSession', 'from pyspark.sql.utils import AnalysisException']",[],[],[],"['pyspark.sql.SparkSession', 'pyspark.sql.utils.AnalysisException']",[],[]
,hello.py,Python Script,6,0,1,0,,[],[],[],[],[],['hello_world'],[]
,pubsub_prj1.py,Python Script,21,0,1,0,,['google.cloud.pubsub_v1'],[],[],[],['google.cloud.pubsub_v1'],['publish_message'],[]
,pubsub_prj2.py,Python Script,25,0,1,0,,['google.cloud.pubsub_v1'],[],[],[],['google.cloud.pubsub_v1'],['callback'],[]
,gcs_to_bq.py,Python Script,47,0,0,0,PySpark Job,"['from pyspark.sql import SparkSession', 'from pyspark.sql.utils import AnalysisException']",[],[],[],"['pyspark.sql.SparkSession', 'pyspark.sql.utils.AnalysisException']",[],[]
,helloworld.py,Python Script,6,0,1,0,,[],[],[],[],[],['hello_world'],[]
,hive_query.sql,SQL Script,13,0,0,3,SQL Query,['sample_db'],[],[],[],[],[],[]
,hive_spark.py,Python Script,22,0,0,0,PySpark Job,['from pyspark.sql import SparkSession'],[],[],"['Hive Table: my_database.my_filtered_table (mode: overwrite)', 'Parquet File: s3a://path/to/output/directory (mode: overwrite)']",['pyspark.sql.SparkSession'],[],[]
,setup_cron.sh,Shell Script,51,0,0,0,,[],['0 2 * * *'],[],[],[],[],[]
,spark_job.py,Python Script,57,1,5,2,PySpark Job,['from pyspark.sql import SparkSession'],[],[],[],['pyspark.sql.SparkSession'],"['__init__', 'load_data_from_hive', 'create_result_table', 'show_data', 'stop_spark_session']",['PySparkHiveExample']
