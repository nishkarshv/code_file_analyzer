source,file_name,type,lines_of_code,class_count,function_count,query_count,purpose,dependencies,triggers,sources,sinks,variables,variable_counts,imports,function_names,class_names
,code_analyzer.py,Python Script,304,0,9,0,,"['os', 're', 'pandas', 'ast', 'croniter.croniter', 'pathlib.Path']",[],[],[],"FILE_TYPES, cron_triggers, cron, cron_expr, variables, lines, var_match, load_match, load_arg, resolved_path, table_match, table_name, save_match, save_arg, save_table_match, bigquery_match, complex_var_match, variable_counts, class_names, function_names, tree, imports, class_count, function_count, module, tables, query_count, content, matches, ext, file_type, file_info, results, file_path, df, repo_path, repo_name, output_csv",38,"['os', 're', 'pandas', 'ast', 'croniter.croniter', 'pathlib.Path']","['count_lines', 'extract_cron_triggers', 'analyze_pyspark', 'extract_variables', 'extract_function_and_class_names', 'extract_imports_and_counts', 'analyze_sql', 'analyze_file', 'analyze_repo']",[]
,code_analyzer_2.py,Python Script,343,0,11,0,,"['os', 're', 'pandas', 'ast', 'croniter.croniter', 'pathlib.Path']",[],[],[],"FILE_TYPES, cron_triggers, content, cron_match, cron_schedule, cron_expr, variables, query_count, lines, sql_query_pattern, sql_queries, var_match, table_match, table_name, resolved_table, load_match, load_arg, resolved_path, save_table_match, save_mode, save_parquet_match, file_path, sql_query_match, sql_query, imports, tree, module, dependencies, variable_counts, class_names, function_names, class_count, function_count, table_matches, ext, file_type, file_info, results, df, repo_path, repo_name, output_csv",42,"['os', 're', 'pandas', 'ast', 'croniter.croniter', 'pathlib.Path']","['count_lines', 'extract_cron_triggers', 'analyze_pyspark', 'extract_imports', 'analyze_python_file', 'extract_variables', 'extract_function_and_class_names', 'extract_imports_and_counts', 'analyze_sql', 'analyze_file', 'analyze_repo']",[]
,python_rules_check.sh,Shell Script,127,0,0,0,,[],[],[],[],"test_file, path, VENV_DIR, CODE_DIR, LOG_DIR, LOG_FILE, DEFAULT_VENV_DIR, DEVELOPMENT_TOOLS",8,[],[],[]
,python_rules_check_2.sh,Shell Script,83,0,0,0,,[],[],[],[],"LOG_FILE, test_file, VENV_DIR, DEVELOPMENT_TOOLS",4,[],[],[]
,deploy.sh,Shell Script,48,0,0,0,,[],[],[],[],,0,[],[],[]
,crud_operations_bq_dag.py,Python Script,74,0,0,0,,"['airflow.DAG', 'airflow.providers.google.cloud.operators.bigquery.BigQueryCreateEmptyDatasetOperator', 'airflow.utils.dates.days_ago']",[],[],[],"default_args, schedule_interval, catchup, create_dataset, task_id, dataset_id, project_id, create_table, table_id, schema_fields, insert_data, configuration, delete_table, deletion_dataset_table, delete_dataset, delete_contents",16,"['airflow.DAG', 'airflow.providers.google.cloud.operators.bigquery.BigQueryCreateEmptyDatasetOperator', 'airflow.utils.dates.days_ago']",[],[]
,gcs_to_bq_dag.py,Python Script,74,0,0,0,,"['airflow.DAG', 'airflow.providers.google.cloud.operators.dataproc.DataprocCreateClusterOperator', 'datetime.datetime']",[],[],[],"default_args, dag_id, start_date, schedule_interval, catchup, create_cluster, task_id, project_id, cluster_name, region, cluster_config, dataproc_job, submit_pyspark_job, job",14,"['airflow.DAG', 'airflow.providers.google.cloud.operators.dataproc.DataprocCreateClusterOperator', 'datetime.datetime']",[],[]
,bq_operations.py,Python Script,98,0,7,0,,"['google.cloud.bigquery', 'time']",[],[],[],"client, project_id, dataset_id, table_id, dataset_ref, dataset, schema, table_ref, table, rows_to_insert, errors, query, query_job, results",14,"['google.cloud.bigquery', 'time']","['create_dataset', 'create_table', 'insert_data', 'query_data', 'update_data', 'delete_data', 'main']",[]
,gcs_to_bq.py,Python Script,49,0,0,0,PySpark Job,"['from pyspark.sql import SparkSession', 'from pyspark.sql.utils import AnalysisException']",[],[],[],"spark, gcs_file_path, temporary_gcs_bucket, bq_table, df",5,"['pyspark.sql.SparkSession', 'pyspark.sql.utils.AnalysisException']",[],[]
,hello.py,Python Script,6,0,1,0,,[],[],[],[],,0,[],['hello_world'],[]
,pubsub_prj1.py,Python Script,21,0,1,0,,['google.cloud.pubsub_v1'],[],[],[],"publisher, topic_path, future",3,['google.cloud.pubsub_v1'],['publish_message'],[]
,pubsub_prj2.py,Python Script,25,0,1,0,,['google.cloud.pubsub_v1'],[],[],[],"subscriber, subscription_path, streaming_pull_future",3,['google.cloud.pubsub_v1'],['callback'],[]
,gcs_to_bq.py,Python Script,47,0,0,0,PySpark Job,"['from pyspark.sql import SparkSession', 'from pyspark.sql.utils import AnalysisException']",[],[],[],"spark, gcs_file_path, temporary_gcs_bucket, bq_table, df",5,"['pyspark.sql.SparkSession', 'pyspark.sql.utils.AnalysisException']",[],[]
,helloworld.py,Python Script,6,0,1,0,,[],[],[],[],a,1,[],['hello_world'],[]
,hive_query.sql,SQL Script,13,0,0,[],SQL Query,[],[],[],[],,0,[],[],[]
,hive_spark.py,Python Script,22,0,0,0,PySpark Job,['from pyspark.sql import SparkSession'],[],[],"['Hive Table: my_database.my_filtered_table (mode: overwrite)', 'Parquet File: s3a://path/to/output/directory (mode: overwrite)']","spark, hive_table, df, df_filtered",4,['pyspark.sql.SparkSession'],[],[]
,setup_cron.sh,Shell Script,51,0,0,0,,[],['0 2 * * *'],[],[],"SPARK_SCRIPT, LOG_FILE, CRON_SCHEDULE, CRON_JOB",4,[],[],[]
,spark_job.py,Python Script,57,1,5,0,PySpark Job,['from pyspark.sql import SparkSession'],[],[],[],"query, df, pyspark_job, hive_table, result_df",5,['pyspark.sql.SparkSession'],"['__init__', 'load_data_from_hive', 'create_result_table', 'show_data', 'stop_spark_session']",['PySparkHiveExample']
